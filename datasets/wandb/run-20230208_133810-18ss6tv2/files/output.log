--------------------------------------------------------------------------------
                     working dir: ./exp/clip_ucf/ViT-B/16/ucf101/20230208_123145
--------------------------------------------------------------------------------
--------------------------------------------------------------------------------
                               Config
{   'data': {   'batch_size': 16,
                'dataset': 'ucf101',
                'gpus': 1,
                'image_tmpl': 'img_{:05d}.jpg',
                'index_bias': 1,
                'input_size': 224,
                'label_list': '/home/shahzaa/Code/ActionCLIP/lists/ucf_labels.csv',
                'modality': 'RGB',
                'num_classes': 101,
                'num_segments': 8,
                'randaug': {'M': 0, 'N': 0},
                'seg_length': 1,
                'split': 1,
                'train_list': '/home/shahzaa/Code/ActionCLIP/train.txt',
                'val_list': '/home/shahzaa/Code/ActionCLIP/test.txt',
                'workers': 8},
    'logging': {'eval_freq': 1, 'print_freq': 10},
    'network': {   'arch': 'ViT-B/16',
                   'describe': None,
                   'drop_out': 0.0,
                   'emb_dropout': 0.0,
                   'fix_img': False,
                   'fix_text': False,
                   'init': True,
                   'sim_header': 'Transf',
                   'type': 'clip_ucf'},
    'pretrain': 'None',
    'resume': None,
    'seed': 1024,
    'solver': {   'clip_gradient': 20,
                  'epoch_offset': 0,
                  'epochs': 50,
                  'evaluate': False,
                  'f_ratio': 10,
                  'loss_type': 'nll',
                  'lr': 5e-06,
                  'lr_decay_factor': 0.1,
                  'lr_decay_step': 15,
                  'lr_warmup_step': 5,
                  'momentum': 0.9,
                  'optim': 'adamw',
                  'ratio': 1,
                  'start_epoch': 0,
                  'type': 'cosine',
                  'weight_decay': 0.2}}
--------------------------------------------------------------------------------
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
dropout used:[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
loading clip pretrained model!
train transforms: [Compose(
    <datasets.transforms_ss.GroupMultiScaleCrop object at 0x7f0c3a285f70>
    <datasets.transforms_ss.GroupRandomHorizontalFlip object at 0x7f0c3a285fa0>
    <datasets.transforms_ss.GroupRandomColorJitter object at 0x7f0c3a285cd0>
    <datasets.transforms_ss.GroupRandomGrayscale object at 0x7f0c3a285f10>
    <datasets.transforms_ss.GroupGaussianBlur object at 0x7f0c3a285dc0>
    <datasets.transforms_ss.GroupSolarization object at 0x7f0c3a285d60>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7f0c3a285e80>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7f0c3a285070>
    <datasets.transforms_ss.GroupNormalize object at 0x7f0c3a285c40>
)]
val transforms: [Compose(
    <datasets.transforms_ss.GroupScale object at 0x7f0c3a285a60>
    <datasets.transforms_ss.GroupCenterCrop object at 0x7f0c3a2854c0>
), Compose(
    <datasets.transforms_ss.Stack object at 0x7f0c3a285a00>
    <datasets.transforms_ss.ToTorchFormatTensor object at 0x7f0c3a285820>
    <datasets.transforms_ss.GroupNormalize object at 0x7f0c3a285670>
)]
layer=6
/home/shahzaa/anaconda3/envs/Action_CLIP/lib/python3.8/site-packages/torchvision/transforms/transforms.py:329: UserWarning: Argument 'interpolation' of type int is deprecated since 0.13 and will be removed in 0.15. Please use InterpolationMode enum.
  warnings.warn(
=========using KL Loss=and has temperature and * bz==========
=========using KL Loss=and has temperature and * bz==========
=> no checkpoint found at 'None'
5e-06
5e-06
5e-05
AdamW
positional_embedding: True
text_projection: True
logit_scale: True
visual.class_embedding: True
visual.positional_embedding: True
visual.proj: True
visual.conv1.weight: True
visual.ln_pre.weight: True
visual.ln_pre.bias: True
visual.transformer.resblocks.0.attn.in_proj_weight: True
visual.transformer.resblocks.0.attn.in_proj_bias: True
visual.transformer.resblocks.0.attn.out_proj.weight: True
visual.transformer.resblocks.0.attn.out_proj.bias: True
visual.transformer.resblocks.0.ln_1.weight: True
visual.transformer.resblocks.0.ln_1.bias: True
visual.transformer.resblocks.0.mlp.c_fc.weight: True
visual.transformer.resblocks.0.mlp.c_fc.bias: True
visual.transformer.resblocks.0.mlp.c_proj.weight: True
visual.transformer.resblocks.0.mlp.c_proj.bias: True
visual.transformer.resblocks.0.ln_2.weight: True
visual.transformer.resblocks.0.ln_2.bias: True
visual.transformer.resblocks.1.attn.in_proj_weight: True
visual.transformer.resblocks.1.attn.in_proj_bias: True
visual.transformer.resblocks.1.attn.out_proj.weight: True
visual.transformer.resblocks.1.attn.out_proj.bias: True
visual.transformer.resblocks.1.ln_1.weight: True
visual.transformer.resblocks.1.ln_1.bias: True
visual.transformer.resblocks.1.mlp.c_fc.weight: True
visual.transformer.resblocks.1.mlp.c_fc.bias: True
visual.transformer.resblocks.1.mlp.c_proj.weight: True
visual.transformer.resblocks.1.mlp.c_proj.bias: True
visual.transformer.resblocks.1.ln_2.weight: True
visual.transformer.resblocks.1.ln_2.bias: True
visual.transformer.resblocks.2.attn.in_proj_weight: True
visual.transformer.resblocks.2.attn.in_proj_bias: True
visual.transformer.resblocks.2.attn.out_proj.weight: True
visual.transformer.resblocks.2.attn.out_proj.bias: True
visual.transformer.resblocks.2.ln_1.weight: True
visual.transformer.resblocks.2.ln_1.bias: True
visual.transformer.resblocks.2.mlp.c_fc.weight: True
visual.transformer.resblocks.2.mlp.c_fc.bias: True
visual.transformer.resblocks.2.mlp.c_proj.weight: True
visual.transformer.resblocks.2.mlp.c_proj.bias: True
visual.transformer.resblocks.2.ln_2.weight: True
visual.transformer.resblocks.2.ln_2.bias: True
visual.transformer.resblocks.3.attn.in_proj_weight: True
visual.transformer.resblocks.3.attn.in_proj_bias: True
visual.transformer.resblocks.3.attn.out_proj.weight: True
visual.transformer.resblocks.3.attn.out_proj.bias: True
visual.transformer.resblocks.3.ln_1.weight: True
visual.transformer.resblocks.3.ln_1.bias: True
visual.transformer.resblocks.3.mlp.c_fc.weight: True
visual.transformer.resblocks.3.mlp.c_fc.bias: True
visual.transformer.resblocks.3.mlp.c_proj.weight: True
visual.transformer.resblocks.3.mlp.c_proj.bias: True
visual.transformer.resblocks.3.ln_2.weight: True
visual.transformer.resblocks.3.ln_2.bias: True
visual.transformer.resblocks.4.attn.in_proj_weight: True
visual.transformer.resblocks.4.attn.in_proj_bias: True
visual.transformer.resblocks.4.attn.out_proj.weight: True
visual.transformer.resblocks.4.attn.out_proj.bias: True
visual.transformer.resblocks.4.ln_1.weight: True
visual.transformer.resblocks.4.ln_1.bias: True
visual.transformer.resblocks.4.mlp.c_fc.weight: True
visual.transformer.resblocks.4.mlp.c_fc.bias: True
visual.transformer.resblocks.4.mlp.c_proj.weight: True
visual.transformer.resblocks.4.mlp.c_proj.bias: True
visual.transformer.resblocks.4.ln_2.weight: True
visual.transformer.resblocks.4.ln_2.bias: True
visual.transformer.resblocks.5.attn.in_proj_weight: True
visual.transformer.resblocks.5.attn.in_proj_bias: True
visual.transformer.resblocks.5.attn.out_proj.weight: True
visual.transformer.resblocks.5.attn.out_proj.bias: True
visual.transformer.resblocks.5.ln_1.weight: True
visual.transformer.resblocks.5.ln_1.bias: True
visual.transformer.resblocks.5.mlp.c_fc.weight: True
visual.transformer.resblocks.5.mlp.c_fc.bias: True
visual.transformer.resblocks.5.mlp.c_proj.weight: True
visual.transformer.resblocks.5.mlp.c_proj.bias: True
visual.transformer.resblocks.5.ln_2.weight: True
visual.transformer.resblocks.5.ln_2.bias: True
visual.transformer.resblocks.6.attn.in_proj_weight: True
visual.transformer.resblocks.6.attn.in_proj_bias: True
visual.transformer.resblocks.6.attn.out_proj.weight: True
visual.transformer.resblocks.6.attn.out_proj.bias: True
visual.transformer.resblocks.6.ln_1.weight: True
visual.transformer.resblocks.6.ln_1.bias: True
visual.transformer.resblocks.6.mlp.c_fc.weight: True
visual.transformer.resblocks.6.mlp.c_fc.bias: True
visual.transformer.resblocks.6.mlp.c_proj.weight: True
visual.transformer.resblocks.6.mlp.c_proj.bias: True
visual.transformer.resblocks.6.ln_2.weight: True
visual.transformer.resblocks.6.ln_2.bias: True
visual.transformer.resblocks.7.attn.in_proj_weight: True
visual.transformer.resblocks.7.attn.in_proj_bias: True
visual.transformer.resblocks.7.attn.out_proj.weight: True
visual.transformer.resblocks.7.attn.out_proj.bias: True
visual.transformer.resblocks.7.ln_1.weight: True
visual.transformer.resblocks.7.ln_1.bias: True
visual.transformer.resblocks.7.mlp.c_fc.weight: True
visual.transformer.resblocks.7.mlp.c_fc.bias: True
visual.transformer.resblocks.7.mlp.c_proj.weight: True
visual.transformer.resblocks.7.mlp.c_proj.bias: True
visual.transformer.resblocks.7.ln_2.weight: True
visual.transformer.resblocks.7.ln_2.bias: True
visual.transformer.resblocks.8.attn.in_proj_weight: True
visual.transformer.resblocks.8.attn.in_proj_bias: True
visual.transformer.resblocks.8.attn.out_proj.weight: True
visual.transformer.resblocks.8.attn.out_proj.bias: True
visual.transformer.resblocks.8.ln_1.weight: True
visual.transformer.resblocks.8.ln_1.bias: True
visual.transformer.resblocks.8.mlp.c_fc.weight: True
visual.transformer.resblocks.8.mlp.c_fc.bias: True
visual.transformer.resblocks.8.mlp.c_proj.weight: True
visual.transformer.resblocks.8.mlp.c_proj.bias: True
visual.transformer.resblocks.8.ln_2.weight: True
visual.transformer.resblocks.8.ln_2.bias: True
visual.transformer.resblocks.9.attn.in_proj_weight: True
visual.transformer.resblocks.9.attn.in_proj_bias: True
visual.transformer.resblocks.9.attn.out_proj.weight: True
visual.transformer.resblocks.9.attn.out_proj.bias: True
visual.transformer.resblocks.9.ln_1.weight: True
visual.transformer.resblocks.9.ln_1.bias: True
visual.transformer.resblocks.9.mlp.c_fc.weight: True
visual.transformer.resblocks.9.mlp.c_fc.bias: True
visual.transformer.resblocks.9.mlp.c_proj.weight: True
visual.transformer.resblocks.9.mlp.c_proj.bias: True
visual.transformer.resblocks.9.ln_2.weight: True
visual.transformer.resblocks.9.ln_2.bias: True
visual.transformer.resblocks.10.attn.in_proj_weight: True
visual.transformer.resblocks.10.attn.in_proj_bias: True
visual.transformer.resblocks.10.attn.out_proj.weight: True
visual.transformer.resblocks.10.attn.out_proj.bias: True
visual.transformer.resblocks.10.ln_1.weight: True
visual.transformer.resblocks.10.ln_1.bias: True
visual.transformer.resblocks.10.mlp.c_fc.weight: True
visual.transformer.resblocks.10.mlp.c_fc.bias: True
visual.transformer.resblocks.10.mlp.c_proj.weight: True
visual.transformer.resblocks.10.mlp.c_proj.bias: True
visual.transformer.resblocks.10.ln_2.weight: True
visual.transformer.resblocks.10.ln_2.bias: True
visual.transformer.resblocks.11.attn.in_proj_weight: True
visual.transformer.resblocks.11.attn.in_proj_bias: True
visual.transformer.resblocks.11.attn.out_proj.weight: True
visual.transformer.resblocks.11.attn.out_proj.bias: True
visual.transformer.resblocks.11.ln_1.weight: True
visual.transformer.resblocks.11.ln_1.bias: True
visual.transformer.resblocks.11.mlp.c_fc.weight: True
visual.transformer.resblocks.11.mlp.c_fc.bias: True
visual.transformer.resblocks.11.mlp.c_proj.weight: True
visual.transformer.resblocks.11.mlp.c_proj.bias: True
visual.transformer.resblocks.11.ln_2.weight: True
visual.transformer.resblocks.11.ln_2.bias: True
visual.ln_post.weight: True
visual.ln_post.bias: True
transformer.resblocks.0.attn.in_proj_weight: True
transformer.resblocks.0.attn.in_proj_bias: True
transformer.resblocks.0.attn.out_proj.weight: True
transformer.resblocks.0.attn.out_proj.bias: True
transformer.resblocks.0.ln_1.weight: True
transformer.resblocks.0.ln_1.bias: True
transformer.resblocks.0.mlp.c_fc.weight: True
transformer.resblocks.0.mlp.c_fc.bias: True
transformer.resblocks.0.mlp.c_proj.weight: True
transformer.resblocks.0.mlp.c_proj.bias: True
transformer.resblocks.0.ln_2.weight: True
transformer.resblocks.0.ln_2.bias: True
transformer.resblocks.1.attn.in_proj_weight: True
transformer.resblocks.1.attn.in_proj_bias: True
transformer.resblocks.1.attn.out_proj.weight: True
transformer.resblocks.1.attn.out_proj.bias: True
transformer.resblocks.1.ln_1.weight: True
transformer.resblocks.1.ln_1.bias: True
transformer.resblocks.1.mlp.c_fc.weight: True
transformer.resblocks.1.mlp.c_fc.bias: True
transformer.resblocks.1.mlp.c_proj.weight: True
transformer.resblocks.1.mlp.c_proj.bias: True
transformer.resblocks.1.ln_2.weight: True
transformer.resblocks.1.ln_2.bias: True
transformer.resblocks.2.attn.in_proj_weight: True
transformer.resblocks.2.attn.in_proj_bias: True
transformer.resblocks.2.attn.out_proj.weight: True
transformer.resblocks.2.attn.out_proj.bias: True
transformer.resblocks.2.ln_1.weight: True
transformer.resblocks.2.ln_1.bias: True
transformer.resblocks.2.mlp.c_fc.weight: True
transformer.resblocks.2.mlp.c_fc.bias: True
transformer.resblocks.2.mlp.c_proj.weight: True
transformer.resblocks.2.mlp.c_proj.bias: True
transformer.resblocks.2.ln_2.weight: True
transformer.resblocks.2.ln_2.bias: True
transformer.resblocks.3.attn.in_proj_weight: True
transformer.resblocks.3.attn.in_proj_bias: True
transformer.resblocks.3.attn.out_proj.weight: True
transformer.resblocks.3.attn.out_proj.bias: True
transformer.resblocks.3.ln_1.weight: True
transformer.resblocks.3.ln_1.bias: True
transformer.resblocks.3.mlp.c_fc.weight: True
transformer.resblocks.3.mlp.c_fc.bias: True
transformer.resblocks.3.mlp.c_proj.weight: True
transformer.resblocks.3.mlp.c_proj.bias: True
transformer.resblocks.3.ln_2.weight: True
transformer.resblocks.3.ln_2.bias: True
transformer.resblocks.4.attn.in_proj_weight: True
transformer.resblocks.4.attn.in_proj_bias: True
transformer.resblocks.4.attn.out_proj.weight: True
transformer.resblocks.4.attn.out_proj.bias: True
transformer.resblocks.4.ln_1.weight: True
transformer.resblocks.4.ln_1.bias: True
transformer.resblocks.4.mlp.c_fc.weight: True
transformer.resblocks.4.mlp.c_fc.bias: True
transformer.resblocks.4.mlp.c_proj.weight: True
transformer.resblocks.4.mlp.c_proj.bias: True
transformer.resblocks.4.ln_2.weight: True
transformer.resblocks.4.ln_2.bias: True
transformer.resblocks.5.attn.in_proj_weight: True
transformer.resblocks.5.attn.in_proj_bias: True
transformer.resblocks.5.attn.out_proj.weight: True
transformer.resblocks.5.attn.out_proj.bias: True
transformer.resblocks.5.ln_1.weight: True
transformer.resblocks.5.ln_1.bias: True
transformer.resblocks.5.mlp.c_fc.weight: True
transformer.resblocks.5.mlp.c_fc.bias: True
transformer.resblocks.5.mlp.c_proj.weight: True
transformer.resblocks.5.mlp.c_proj.bias: True
transformer.resblocks.5.ln_2.weight: True
transformer.resblocks.5.ln_2.bias: True
transformer.resblocks.6.attn.in_proj_weight: True
transformer.resblocks.6.attn.in_proj_bias: True
transformer.resblocks.6.attn.out_proj.weight: True
transformer.resblocks.6.attn.out_proj.bias: True
transformer.resblocks.6.ln_1.weight: True
transformer.resblocks.6.ln_1.bias: True
transformer.resblocks.6.mlp.c_fc.weight: True
transformer.resblocks.6.mlp.c_fc.bias: True
transformer.resblocks.6.mlp.c_proj.weight: True
transformer.resblocks.6.mlp.c_proj.bias: True
transformer.resblocks.6.ln_2.weight: True
transformer.resblocks.6.ln_2.bias: True
transformer.resblocks.7.attn.in_proj_weight: True
transformer.resblocks.7.attn.in_proj_bias: True
transformer.resblocks.7.attn.out_proj.weight: True
transformer.resblocks.7.attn.out_proj.bias: True
transformer.resblocks.7.ln_1.weight: True
transformer.resblocks.7.ln_1.bias: True
transformer.resblocks.7.mlp.c_fc.weight: True
transformer.resblocks.7.mlp.c_fc.bias: True
transformer.resblocks.7.mlp.c_proj.weight: True
transformer.resblocks.7.mlp.c_proj.bias: True
transformer.resblocks.7.ln_2.weight: True
transformer.resblocks.7.ln_2.bias: True
transformer.resblocks.8.attn.in_proj_weight: True
transformer.resblocks.8.attn.in_proj_bias: True
transformer.resblocks.8.attn.out_proj.weight: True
transformer.resblocks.8.attn.out_proj.bias: True
transformer.resblocks.8.ln_1.weight: True
transformer.resblocks.8.ln_1.bias: True
transformer.resblocks.8.mlp.c_fc.weight: True
transformer.resblocks.8.mlp.c_fc.bias: True
transformer.resblocks.8.mlp.c_proj.weight: True
transformer.resblocks.8.mlp.c_proj.bias: True
transformer.resblocks.8.ln_2.weight: True
transformer.resblocks.8.ln_2.bias: True
transformer.resblocks.9.attn.in_proj_weight: True
transformer.resblocks.9.attn.in_proj_bias: True
transformer.resblocks.9.attn.out_proj.weight: True
transformer.resblocks.9.attn.out_proj.bias: True
transformer.resblocks.9.ln_1.weight: True
transformer.resblocks.9.ln_1.bias: True
transformer.resblocks.9.mlp.c_fc.weight: True
transformer.resblocks.9.mlp.c_fc.bias: True
transformer.resblocks.9.mlp.c_proj.weight: True
transformer.resblocks.9.mlp.c_proj.bias: True
transformer.resblocks.9.ln_2.weight: True
transformer.resblocks.9.ln_2.bias: True
transformer.resblocks.10.attn.in_proj_weight: True
transformer.resblocks.10.attn.in_proj_bias: True
transformer.resblocks.10.attn.out_proj.weight: True
transformer.resblocks.10.attn.out_proj.bias: True
transformer.resblocks.10.ln_1.weight: True
transformer.resblocks.10.ln_1.bias: True
transformer.resblocks.10.mlp.c_fc.weight: True
transformer.resblocks.10.mlp.c_fc.bias: True
transformer.resblocks.10.mlp.c_proj.weight: True
transformer.resblocks.10.mlp.c_proj.bias: True
transformer.resblocks.10.ln_2.weight: True
transformer.resblocks.10.ln_2.bias: True
transformer.resblocks.11.attn.in_proj_weight: True
transformer.resblocks.11.attn.in_proj_bias: True
transformer.resblocks.11.attn.out_proj.weight: True
transformer.resblocks.11.attn.out_proj.bias: True
transformer.resblocks.11.ln_1.weight: True
transformer.resblocks.11.ln_1.bias: True
transformer.resblocks.11.mlp.c_fc.weight: True
transformer.resblocks.11.mlp.c_fc.bias: True
transformer.resblocks.11.mlp.c_proj.weight: True
transformer.resblocks.11.mlp.c_proj.bias: True
transformer.resblocks.11.ln_2.weight: True
transformer.resblocks.11.ln_2.bias: True
token_embedding.weight: True
ln_final.weight: True
ln_final.bias: True
  0%|                                                                                                                                               | 0/596 [00:00<?, ?it/s]/home/shahzaa/anaconda3/envs/Action_CLIP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
/home/shahzaa/anaconda3/envs/Action_CLIP/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:163: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
/home/shahzaa/anaconda3/envs/Action_CLIP/lib/python3.8/site-packages/torch/nn/functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
  0%|â–                                                                                                                                    | 1/596 [00:49<8:15:03, 49.92s/it]/home/shahzaa/anaconda3/envs/Action_CLIP/lib/python3.8/site-packages/torch/nn/functional.py:2916: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.
  warnings.warn(
